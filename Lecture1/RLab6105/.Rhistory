docs <- tm_map(docs, stripWhitespace)
inspect(docs[1]) # Check to see if it worked.
## To Finish
## Be sure to use the following script once you have completed preprocessing.
## This tells R to treat your preprocessed documents as text documents.
docs <- tm_map(docs, PlainTextDocument)
## This is the end of the preprocessing stage! Congratulations, you've done what data scientists spend 90% of their time on: data cleansing!
##
## PART II:
## Staging the Data
##
## create a document term matrix.
## This is what you will be using from this point on.
dtm <- DocumentTermMatrix(docs)
library(caret)
setwd("M:/NEU folder/Data Science/Lecture1/RLab6105/programs")
# rename the dataset
dataset <- LiverPatientDataset
# define the filename
filename <- "LiverPatientDataset.csv"
# load the CSV file from the local directory
dataset <- read.csv("../data/filename", header=FALSE)
# load the CSV file from the local directory
dataset <- read.csv("data/filename", header=FALSE)
getwd()
setwd("M:/NEU folder/Data Science/Lecture1/RLab6105")
setwd("M:/NEU folder/Data Science/Lecture1/RLab6105")
# define the filename
filename <- "LiverPatientDataset.csv"
# load the CSV file from the local directory
dataset <- read.csv("data/filename", header=FALSE)
# load the CSV file from the local directory
dataset <- read.csv("data/LiverPatientDataset.csv", header=FALSE)
# set the column names in the dataset
colnames(dataset) <- c("Age","TB","DB","Alkphos","Sgpt","Sgot","TP","ALB","A/G Ratio","Selector")
# create a list of 80% of the rows in the original dataset we can use for training
validation_index <- createDataPartition(dataset$Species, p=0.80, list=FALSE)
# create a list of 80% of the rows in the original dataset we can use for training
validation_index <- createDataPartition(dataset$Selector, p=0.80, list=FALSE)
dataset
# set the column names in the dataset
colnames(dataset) <- c("Age","TB","DB","Alkphos","Sgpt","Sgot","TP","ALB","A/G Ratio","Selector")
dataset
# create a list of 80% of the rows in the original dataset we can use for training
validation_index <- createDataPartition(dataset$Selector, p=0.80, list=FALSE)
# create a list of 80% of the rows in the original dataset we can use for training
validation_index <- createDataPartition(dataset$Selector, p=0.80, list=TRUE)
# create a list of 80% of the rows in the original dataset we can use for training
validation_index <- createDataPartition(dataset$Selector, p=0.80, list=FALSE,na.rm=TRUE)
# create a list of 80% of the rows in the original dataset we can use for training
validation_index <- createDataPartition(dataset$Selector, p=0.80, list=FALSE)
# select 20% of the data for validation
validation <- dataset[-validation_index,]
# create a list of 80% of the rows in the original dataset we can use for training
validation_index <- createDataPartition(dataset$Selector, p=0.80, list=FALSE,na.action=na.omit)
# select 20% of the data for validation
validation <- dataset[-validation_index,]
library(caret)
# load the CSV file from the local directory
dataset <- read.csv("data/LiverPatientDataset.csv", header=FALSE)
# set the column names in the dataset
colnames(dataset) <- c("Age","TB","DB","Alkphos","Sgpt","Sgot","TP","ALB","A/G Ratio","Selector")
dataset
# create a list of 80% of the rows in the original dataset we can use for training
validation_index <- createDataPartition(dataset$Selector, p=0.80, list=FALSE)
sapply(dataset,class)
is.na(dataset)
# create a list of 80% of the rows in the original dataset we can use for training
validation_index <- createDataPartition(dataset$Selector, p=0.80, list=FALSE)
# select 20% of the data for validation
validation <- dataset[-validation_index,]
# use the remaining 80% of data to training and testing the models
dataset <- dataset[validation_index,]
FALSE
FALSE
# load the CSV file from the local directory
dataset <- read.csv("data/LiverPatientDataset.csv", header=FALSE)
# set the column names in the dataset
colnames(dataset) <- c("Age","TB","DB","Alkphos","Sgpt","Sgot","TP","ALB","A/G Ratio","Selector")
dataset
# create a list of 80% of the rows in the original dataset we can use for training
validation_index <- createDataPartition(dataset$Selector, p=0.80, list=FALSE)
na.omit(dataset)
# create a list of 80% of the rows in the original dataset we can use for training
validation_index <- createDataPartition(dataset$Selector, p=0.80, list=FALSE)
dataset <- na.omit(dataset)
# create a list of 80% of the rows in the original dataset we can use for training
validation_index <- createDataPartition(dataset$Selector, p=0.80, list=FALSE)
# select 20% of the data for validation
validation <- dataset[-validation_index,]
# use the remaining 80% of data to training and testing the models
dataset <- dataset[validation_index,]
# dimensions of dataset
dim(dataset)
# list types for each attribute
sapply(dataset, class)
# take a peek at the first 5 rows of the data
head(dataset)
# list the levels for the class
levels(dataset$Species)
Selector
# dimensions of dataset
dim(dataset)
# list the levels for the class
levels(dataset$Selector)
# summarize the class distribution
percentage <- prop.table(table(dataset$Species)) * 100
cbind(freq=table(dataset$Species), percentage=percentage)
# summarize attribute distributions
summary(dataset)
# split input and output
x <- dataset[,1:4]
# split input and output
x <- dataset[,1:9]
y <- dataset[,10]
# boxplot for each attribute on one image
par(mfrow=c(1,4))
for(i in 1:4) {
boxplot(x[,i], main=names(iris)[i])
}
# boxplot for each attribute on one image
par(mfrow=c(1,9))
for(i in 1:9) {
boxplot(x[,i], main=names(iris)[i])
}
# boxplot for each attribute on one image
par(mfrow=c(1,9))
for(i in 1:9) {
boxplot(x[,i], main=names(dataset)[i])
}
# boxplot for each attribute on one image
par(mfrow=c(1,6))
for(i in 1:6) {
boxplot(x[,i], main=names(dataset)[i])
}
# barplot for class breakdown
plot(y)
# scatterplot matrix
install.packages("ISLR")
library(ISLR); library(ggplot2); library(caret);
featurePlot(x=x, y=y)
featurePlot(x=x, y=y, plot="pairs")
# box and whisker plots for each attribute
featurePlot(x=x, y=y, plot="box")
# density plots for each attribute by class value
scales <- list(x=list(relation="free"), y=list(relation="free"))
featurePlot(x=x, y=y, plot="density", scales=scales)
5.1 Test Harness
featurePlot(x=x, y=y, plot="density", scales=scales)
5.1 Test Harness
We will 10-fold crossvalidation to estimate accuracy.
# Run algorithms using 10-fold cross validation
control <- trainControl(method="cv", number=10)
metric <- "Accuracy"
library(e1071)
set.seed(7)
fit.lda <- train(Species~., data=dataset, method="lda", metric=metric, trControl=control)
fit.lda <- train(Selector~., data=dataset, method="lda", metric=metric, trControl=control)
# b) nonlinear algorithms
# kNN
set.seed(7)
fit.knn <- train(Species~., data=dataset, method="knn", metric=metric, trControl=control)
# b) nonlinear algorithms
# kNN
set.seed(7)
fit.knn <- train(Selector~., data=dataset, method="knn", metric=metric, trControl=control)
library(caret)
# load the CSV file from the local directory
dataset <- read.csv("data/LiverPatientDataset.csv", header=FALSE)
# set the column names in the dataset
colnames(dataset) <- c("Age","TB","DB","Alkphos","Sgpt","Sgot","TP","ALB","A/G Ratio","Selector")
dataset <- na.omit(dataset)
# create a list of 80% of the rows in the original dataset we can use for training
validation_index <- createDataPartition(dataset$Selector, p=0.80, list=FALSE)
# select 20% of the data for validation
validation <- dataset[-validation_index,]
# use the remaining 80% of data to training and testing the models
dataset <- dataset[validation_index,]
# dimensions of dataset
dim(dataset)
# list types for each attribute
sapply(dataset, class)
# take a peek at the first 5 rows of the data
head(dataset)
# list the levels for the class
levels(dataset$Selector)
# summarize the class distribution
percentage <- prop.table(table(dataset$Species)) * 100
cbind(freq=table(dataset$Species), percentage=percentage)
# summarize attribute distributions
summary(dataset)
# split input and output
x <- dataset[,1:9]
y <- dataset[,10]
# boxplot for each attribute on one image
par(mfrow=c(1,6))
for(i in 1:6) {
boxplot(x[,i], main=names(dataset)[i])
}
# barplot for class breakdown
plot(y)
# scatterplot matrix
install.packages("ISLR")
install.packages("ISLR")
library(ISLR); library(ggplot2); library(caret);
featurePlot(x=x, y=y)
featurePlot(x=x, y=y, plot="pairs")
# box and whisker plots for each attribute
featurePlot(x=x, y=y, plot="box")
# density plots for each attribute by class value
scales <- list(x=list(relation="free"), y=list(relation="free"))
featurePlot(x=x, y=y, plot="density", scales=scales)
# Run algorithms using 10-fold cross validation
control <- trainControl(method="cv", number=10)
metric <- "Accuracy"
# a) linear algorithms
install.packages("e1071")
install.packages("e1071")
library(e1071)
set.seed(7)
fit.lda <- train(Selector~., data=dataset, method="lda", metric=metric, trControl=control)
# b) nonlinear algorithms
# kNN
set.seed(7)
fit.knn <- train(Selector~., data=dataset, method="knn", metric=metric, trControl=control)
fit.knn <- train(Selector~., data=dataset, method="knn", trControl=control)
fit.lda <- train(Selector~., data=dataset, method="lda", trControl=control)
# b) nonlinear algorithms
# kNN
set.seed(7)
fit.knn <- train(Selector~., data=dataset, method="knn", metric=metric, trControl=control)
# c) advanced algorithms
# Random Forest
set.seed(7)
fit.rf <- train(Selector~., data=dataset, method="rf", metric=metric, trControl=control)
5.3 Select Best Model
fit.rf <- train(Selector~., data=dataset, method="rf", trControl=control)
5.3 Select Best Model
# summarize accuracy of models
results <- resamples(list(lda=fit.lda, knn=fit.knn, rf=fit.rf))
set.seed(7)
fit.adaboost <- train(Selector~., data=dataset, method="adaBoost", trControl=control)
set.seed(7)
fit.adaboost <- train(Selector~., data=dataset, method="adaboost", trControl=control)
fit.adaboost <- train(Selector~., data=dataset, method="adaboost",metric=metric, trControl=control)
library(e1071)
set.seed(7)
fit.svnLinear2<- train(Selector~., data=dataset, method="svnLinear2",metric=metric, trControl=control)
library(e1071)
set.seed(7)
fit.svnLinear2<- train(Selector~., data=dataset, method="svnLinear2",metric=metric, trControl=control)
library(e1071)
set.seed(7)
fit.lr<- train(Selector~., data=dataset, method="lr",metric=metric, trControl=control)
library(e1071)
set.seed(7)
fit.lr<- train(Selector~., data=dataset, method="cv",metric=metric, trControl=control)
set.seed(7)
fit.lr<- train(Selector~., data=dataset, method="rf",metric=metric, trControl=control)
fit.lr<- train(Selector~., data=dataset, method="rf", trControl=control)
set.seed(7)
fit.lda<- train(Selector~., data=dataset, method="lda", trControl=control)
set.seed(7)
fit.lda<- train(Selector~., data=dataset, method="svmLinear", trControl=control)
# b) nonlinear algorithms
# kNN
set.seed(7)
fit.knn <- train(Selector~., data=dataset, method="knn", metric=metric, trControl=control)
# c) advanced algorithms
# Random Forest
set.seed(7)
fit.rf <- train(Selector~., data=dataset, method="rf", trControl=control)
5.3 Select Best Model
# summarize accuracy of models
results <- resamples(list(lda=fit.lda, knn=fit.knn, rf=fit.rf))
summary(results)
# compare accuracy of models
dotplot(results)
# summarize Best Model
print(fit.lda)
# estimate skill of LDA on the validation dataset
predictions <- predict(fit.lda, validation)
confusionMatrix(predictions, validation$Species)
confusionMatrix(predictions, validation$Selector)
# estimate skill of LDA on the validation dataset
predictions <- predict(fit.rf, validation)
confusionMatrix(predictions, validation$Selector)
# estimate skill of LDA on the validation dataset
predictions <- predict(fit.rf, validation)
predictions
confusionMatrix(predictions, validation$Selector)
library(caret)
# load the CSV file from the local directory
dataset <- read.csv("data/LiverPatientDataset.csv", header=FALSE)
# set the column names in the dataset
colnames(dataset) <- c("Age","TB","DB","Alkphos","Sgpt","Sgot","TP","ALB","A/G Ratio","Selector")
dataset <- na.omit(dataset)
# create a list of 80% of the rows in the original dataset we can use for training
validation_index <- createDataPartition(dataset$Selector, p=0.80, list=FALSE)
# select 20% of the data for validation
validation <- dataset[-validation_index,]
# use the remaining 80% of data to training and testing the models
dataset <- dataset[validation_index,]
# dimensions of dataset
dim(dataset)
# list types for each attribute
sapply(dataset, class)
# take a peek at the first 5 rows of the data
head(dataset)
# list the levels for the class
levels(dataset$Selector)
# summarize the class distribution
percentage <- prop.table(table(dataset$Species)) * 100
cbind(freq=table(dataset$Species), percentage=percentage)
# summarize attribute distributions
summary(dataset)
library(caret)
# load the CSV file from the local directory
dataset <- read.csv("data/LiverPatientDataset.csv", header=FALSE)
# set the column names in the dataset
colnames(dataset) <- c("Age","TB","DB","Alkphos","Sgpt","Sgot","TP","ALB","A/G Ratio","Selector")
dataset <- na.omit(dataset)
# create a list of 80% of the rows in the original dataset we can use for training
validation_index <- createDataPartition(dataset$Selector, p=0.80, list=FALSE)
# select 20% of the data for validation
validation <- dataset[-validation_index,]
# use the remaining 80% of data to training and testing the models
dataset <- dataset[validation_index,]
# dimensions of dataset
dim(dataset)
# list types for each attribute
sapply(dataset, class)
# take a peek at the first 5 rows of the data
head(dataset)
# list the levels for the class
levels(dataset$Selector)
# summarize the class distribution
percentage <- prop.table(table(dataset$Selector)) * 100
cbind(freq=table(dataset$Selector), percentage=percentage)
# summarize attribute distributions
summary(dataset)
# split input and output
x <- dataset[,1:9]
y <- dataset[,10]
# boxplot for each attribute on one image
par(mfrow=c(1,6))
for(i in 1:6) {
boxplot(x[,i], main=names(dataset)[i])
}
# barplot for class breakdown
plot(y)
# scatterplot matrix
install.packages("ISLR")
install.packages("ISLR")
library(ISLR); library(ggplot2); library(caret);
featurePlot(x=x, y=y)
featurePlot(x=x, y=y, plot="pairs")
# box and whisker plots for each attribute
featurePlot(x=x, y=y, plot="box")
# density plots for each attribute by class value
scales <- list(x=list(relation="free"), y=list(relation="free"))
featurePlot(x=x, y=y, plot="density", scales=scales)
# Run algorithms using 10-fold cross validation
control <- trainControl(method="cv", number=10)
metric <- "Accuracy"
# a) lSVM Linear Algorithm
install.packages("e1071")
install.packages("e1071")
library(e1071)
set.seed(7)
fit.svmLinear<- train(Selector~., data=dataset, method="svmLinear", trControl=control)
# b) nonlinear algorithms
# kNN
set.seed(7)
fit.knn <- train(Selector~., data=dataset, method="knn", metric=metric, trControl=control)
# c) advanced algorithms
# Random Forest
set.seed(7)
fit.knn <- train(Selector~., data=dataset, method="knn", trControl=control)
# c) advanced algorithms
# Random Forest
set.seed(7)
fit.rf <- train(Selector~., data=dataset, method="rf", trControl=control)
# summarize accuracy of models
results <- resamples(list(svmLinear=fit.svmLinear, knn=fit.knn, rf=fit.rf))
summary(results)
# compare accuracy of models
dotplot(results)
# summarize Best Model
print(fit.rf)
# estimate skill of LDA on the validation dataset
predictions <- predict(fit.rf, validation)
confusionMatrix(predictions, validation$Selector)
confusionMatrix(predictions, validation$Selector,,na.action = na.pass)
confusionMatrix(predictions, validation)
predictions <-na.omit(predictions)
confusionMatrix(predictions, validation)
confusionMatrix(table(predictions, validation))
# estimate skill of LDA on the validation dataset
predictions <- predict(fit.rf, validation)
confusionMatrix(table(predictions, validation))
levels(predictions)
levels(validation)
levels(validation_index)
confusionMatrix(table(predictions, validation))
confusionMatrix(predictions, validation)
confusionMatrix(predictions, validation$Selector)
confusionMatrix(
factor(predictions, levels = 1:5),
factor(validation$Selector, levels = 1:5)
)
# estimate skill of LDA on the validation dataset
predictions <- predict(fit.rf, validation)
levels(validation_index)
confusionMatrix(
factor(predictions, levels = 1:5),
factor(validation$Selector, levels = 1:5)
)
confusionMatrix(predictions, validation$Selector)
confusionMatrix(predictions, as.factor(validation$Selector))
confusionMatrix(as.factor(predictions), as.factor(validation$Selector))
# estimate skill of LDA on the validation dataset
predictions <- predict(fit.rf, validation)
confusionMatrix(predictions, validation$Selector)
# estimate skill of LDA on the validation dataset
predictions <- predict(fit.knn, validation)
confusionMatrix(predictions, validation$Selector)
str(predictions)
library(caret)
# load the CSV file from the local directory
dataset <- read.csv("data/LiverPatientDataset.csv", header=FALSE)
# set the column names in the dataset
colnames(dataset) <- c("Age","TB","DB","Alkphos","Sgpt","Sgot","TP","ALB","A/G Ratio","Selector")
dataset <- na.omit(dataset)
# create a list of 80% of the rows in the original dataset we can use for training
validation_index <- createDataPartition(dataset$Selector, p=0.80, list=FALSE)
# select 20% of the data for validation
validation <- dataset[-validation_index,]
# use the remaining 80% of data to training and testing the models
dataset <- dataset[validation_index,]
# dimensions of dataset
dim(dataset)
# list types for each attribute
sapply(dataset, class)
# take a peek at the first 5 rows of the data
head(dataset)
# list the levels for the class
levels(dataset$Selector)
# list the levels for the class
levels(dataset$Selector)
# list the levels for the class
levels(dataset$Selector)
# summarize the class distribution
percentage <- prop.table(table(dataset$Selector)) * 100
cbind(freq=table(dataset$Selector), percentage=percentage)
# summarize attribute distributions
summary(dataset)
# split input and output
x <- dataset[,1:9]
y <- dataset[,10]
# boxplot for each attribute on one image
par(mfrow=c(1,6))
for(i in 1:6) {
boxplot(x[,i], main=names(dataset)[i])
}
# barplot for class breakdown
plot(y)
# scatterplot matrix
install.packages("ISLR")
install.packages("ISLR")
library(ISLR); library(ggplot2); library(caret);
featurePlot(x=x, y=y)
featurePlot(x=x, y=y, plot="pairs")
# box and whisker plots for each attribute
featurePlot(x=x, y=y, plot="box")
# density plots for each attribute by class value
scales <- list(x=list(relation="free"), y=list(relation="free"))
featurePlot(x=x, y=y, plot="density", scales=scales)
# Run algorithms using 10-fold cross validation
control <- trainControl(method="cv", number=10)
metric <- "Accuracy"
# a) lSVM Linear Algorithm
install.packages("e1071")
install.packages("e1071")
library(e1071)
set.seed(7)
fit.svmLinear<- train(Selector~., data=dataset, method="svmLinear", trControl=control)
# b) nonlinear algorithms
# kNN
set.seed(7)
fit.knn <- train(Selector~., data=dataset, method="knn", trControl=control)
# c) advanced algorithms
# Random Forest
set.seed(7)
fit.rf <- train(Selector~., data=dataset, method="rf", trControl=control)
# summarize accuracy of models
results <- resamples(list(svmLinear=fit.svmLinear, knn=fit.knn, rf=fit.rf))
summary(results)
# compare accuracy of models
dotplot(results)
# summarize Best Model
print(fit.knn)
# estimate skill of LDA on the validation dataset
predictions <- predict(fit.knn, validation)
str(predictions)
confusionMatrix(predictions, validation$Selector)
table(factor(predictions, levels=min(validation):max(validation)),
factor(validation, levels=min(validation):max(validation)))
table(factor(predictions, levels=min(validation$Selector):max(validation$Selector)),
factor(validation$Selector, levels=min(validation$Selector):max(validation$Selector)))
confusionMatrix(table(factor(predictions, levels=min(validation$Selector):max(validation$Selector)),
factor(validation$Selector, levels=min(validation$Selector):max(validation$Selector))))
